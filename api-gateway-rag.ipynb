{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/madeeltariq/api-gateway-rag?scriptVersionId=214156681\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# *Installing the required libraries*# ","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport os\n\ndef install_package(package_name):\n    try:\n        # Check if the package is installed\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"show\", package_name])\n        print(f\"{package_name} is already installed.\")\n    except subprocess.CalledProcessError:\n        # If package is not installed, install it\n        print(f\"Installing {package_name}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n        print(f\"{package_name} installation completed.\")\n\ndef install_ngrok():\n    # Check if ngrok is already downloaded and extracted\n    if not os.path.exists('ngrok'):\n        print(\"Downloading and installing ngrok...\")\n        subprocess.run(['wget', '-q', '-O', 'ngrok.zip', 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip'])\n        subprocess.run(['unzip', '-o', 'ngrok.zip'])\n        print(\"ngrok installed successfully.\")\n    else:\n        print(\"ngrok is already installed.\")\n\n# Installing packages step-by-step\ninstall_package(\"flask\")\ninstall_package(\"transformers\")\ninstall_package(\"pinecone-client\")\ninstall_package(\"python-dotenv\")\ninstall_package(\"flask-cors\")\ninstall_package(\"pyngrok\")\n\n# Install ngrok separately\ninstall_ngrok()\n\nprint(\"Done With All installations >>>>>>>>>>>>>>>>> Move Forward\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:13:19.724532Z","iopub.execute_input":"2024-12-21T13:13:19.724882Z","iopub.status.idle":"2024-12-21T13:13:42.494326Z","shell.execute_reply.started":"2024-12-21T13:13:19.724852Z","shell.execute_reply":"2024-12-21T13:13:42.493599Z"}},"outputs":[{"name":"stdout","text":"flask is already installed.\ntransformers is already installed.\nInstalling pinecone-client...\npinecone-client installation completed.\nInstalling python-dotenv...\npython-dotenv installation completed.\nInstalling flask-cors...\nflask-cors installation completed.\nInstalling pyngrok...\npyngrok installation completed.\nDownloading and installing ngrok...\nngrok installed successfully.\nDone With All installations >>>>>>>>>>>>>>>>> Move Forward\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports in here","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Code cell for Entering the ngrok API key For Ngrok implementation**","metadata":{}},{"cell_type":"code","source":"import os\nimport getpass  # Import the getpass module to hide input text\n\n# Function to create/update .env file with ngrok API key\ndef set_ngrok_api_key():\n    # Ask user for the ngrok API key without displaying the input\n    ngrok_api_key = getpass.getpass(\"Please enter your ngrok API key: \")\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the key; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    \n    print(f\"ngrok API key added to {env_file}\")\n\n# Function to create/update .env file with Pinecone credentials\ndef set_pinecone_credentials():\n    # Ask user for the Pinecone API key without displaying the input\n    pinecone_api_key = getpass.getpass(\"Please enter your Pinecone API key: \")\n\n    # Ask user for the Pinecone environment, with a default value\n    pinecone_env = input(\"Please enter your Pinecone environment (e.g., us-west1-gcp) [default: us-east-1]: \") or \"us-east-1\"\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the keys; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    \n    print(f\"Pinecone credentials added to {env_file}\")\n\n# Call the functions to set the API key and credentials\nset_ngrok_api_key()\nset_pinecone_credentials()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:13:58.227058Z","iopub.execute_input":"2024-12-21T13:13:58.227346Z","iopub.status.idle":"2024-12-21T13:14:31.08388Z","shell.execute_reply.started":"2024-12-21T13:13:58.227323Z","shell.execute_reply":"2024-12-21T13:14:31.082952Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Please enter your ngrok API key:  ········\n"},{"name":"stdout","text":"ngrok API key added to .env\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Please enter your Pinecone API key:  ········\nPlease enter your Pinecone environment (e.g., us-west1-gcp) [default: us-east-1]:  \n"},{"name":"stdout","text":"Pinecone credentials added to .env\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv  # Import the dotenv package to load .env file\nfrom pinecone import Pinecone\n\n# Function to get vector from Pinecone database based on ID\ndef get_vector_from_pinecone(file_id, index_name=\"vecotr\", namespace_name=\"newCheck\", pinecone_env=\"us-east-1\"):\n    # Load environment variables from .env file\n    load_dotenv()  # This will load the variables from the .env file into the environment\n\n    # Load Pinecone API key from the environment variables\n    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n    if not pinecone_api_key:\n        print(\"Check if ENV variables are set\")\n\n    # Ensure the API key is not None\n    if not pinecone_api_key:\n        # print(\"Error: Pinecone API key is missing from the environment variables.\")\n        return None\n\n    # Initialize Pinecone client\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        print(f\"Pinecone client initialized with environment: {pinecone_env}\")\n    except Exception as e:\n        print(f\"Error initializing Pinecone client: {str(e)}\")\n        return None\n\n    # Initialize the index\n    try:\n        index = pc.Index(index_name)\n    except Exception as e:\n        print(f\"Error initializing Pinecone index: {str(e)}\")\n        return None\n\n    # Query Pinecone to retrieve vector for the specific ID\n    try:\n        print(f\"Retrieving vector for file ID: {file_id} from Pinecone...\")\n\n        # Querying Pinecone index for the specific ID using metadata filtering\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            id=file_id,\n            top_k=1,\n            include_values=True,\n            include_metadata=True\n        )\n\n        # Check if result contains matches\n        if response and response.get(\"matches\"):\n            # Display the first few vectors based on top_k\n            for idx, vector_data in enumerate(response[\"matches\"]):\n                # print(f\"Vector {idx + 1} ID: {vector_data['id']}\")\n                print(f\"Pinecone Index: {index_name}\")\n                print(f\"Pinecone Env: {pinecone_env}\")\n                # print(f\"Metadata (fileId): {vector_data['metadata']['fileId']}\")\n                # print(f\"Metadata (text): {vector_data['metadata']['text'][:100]}...\")  # Showing first 100 chars of text\n                print(f\"Vector Values: {vector_data['values'][:5]}...\")  # Showing first 5 values of the vector\n                print(f\"Size of Vector Values: {len(vector_data['values'])}\")  # Printing the size of the vector values\n            return response[\"matches\"]\n\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving vector for file ID {file_id}: {str(e)}\")\n        return None\n\n# Example usage of the function\nfile_id = \"676594520c3eb8c3272efa2c\"  # Replace with the file ID you want to query\nindex_name = \"vecotr\"  # The Pinecone index name\nnamespace_name = \"newCheck\"  # The namespace in Pinecone\npinecone_env = \"us-east-1\"  # The Pinecone environment\n\nvector = get_vector_from_pinecone(file_id, index_name=index_name, namespace_name=namespace_name, pinecone_env=pinecone_env)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:17:28.066042Z","iopub.execute_input":"2024-12-21T13:17:28.066327Z","iopub.status.idle":"2024-12-21T13:17:28.284329Z","shell.execute_reply.started":"2024-12-21T13:17:28.066299Z","shell.execute_reply":"2024-12-21T13:17:28.283484Z"}},"outputs":[{"name":"stdout","text":"Pinecone client initialized with environment: us-east-1\nRetrieving vector for file ID: 676594520c3eb8c3272efa2c from Pinecone...\nPinecone Index: vecotr\nPinecone Env: us-east-1\nVector Values: [0.029296875, -0.00984191895, -0.0322265625, -0.0369262695, 0.0473632812]...\nSize of Vector Values: 1024\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# To test this one is ","metadata":{}},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv  # For loading .env file\nfrom pinecone import Pinecone\nfrom transformers import pipeline\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize the Pinecone API key and environment from .env\npinecone_api_key = os.getenv('PINECONE_API_KEY')\npinecone_env = os.getenv('PINECONE_ENV', 'us-east-1')\n\n# Initialize Pinecone client\ndef initialize_pinecone():\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        print(f\"Pinecone client initialized with environment: {pinecone_env}\")\n        return pc\n    except Exception as e:\n        print(f\"Error initializing Pinecone client: {str(e)}\")\n        return None\n\n# Retrieve context from Pinecone\ndef get_context_from_pinecone(file_id, index_name=\"vecotr\", namespace_name=\"newCheck\"):\n    try:\n        pc = initialize_pinecone()\n        if not pc:\n            return None\n\n        index = pc.Index(index_name)\n\n        # Query Pinecone to retrieve vector for the specific ID\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            id=file_id,\n            top_k=1,\n            include_values=True,\n            include_metadata=True\n        )\n\n        if response and response.get(\"matches\"):\n            context = response[\"matches\"][0]['metadata']['text']  # Assuming the text is stored in metadata\n            print(f\"Context for file {file_id}: {context[:100]}...\")  # Display first 100 characters\n            return context\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving context for file ID {file_id}: {str(e)}\")\n        return None\n\n# Initialize the GPT-J model from Hugging Face\ndef initialize_model():\n    generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-j-6B\")\n    return generator\n\n# Generate response using the model and context\ndef generate_response(user_query, context, model):\n    prompt = f\"Question: {user_query}\\nContext: {context}\\nAnswer:\"\n    print(f\"Prompt: {prompt[:200]}...\")  # Displaying first 200 chars of the prompt for debugging\n    response = model(prompt, max_length=150, num_return_sequences=1)\n    return response[0]['generated_text']\n\n# Function to maintain the session\ndef chat_with_model(file_id, user_query):\n    context = get_context_from_pinecone(file_id)\n    if not context:\n        return \"Sorry, I couldn't retrieve the relevant context.\"\n\n    model = initialize_model()\n    \n    # Generate the response based on the user query and context\n    response = generate_response(user_query, context, model)\n    \n    return response\n\n# Example usage: The user asks a question and the system provides an answer\nfile_id = \"676594520c3eb8c3272efa2c\"  # Replace with the file ID you want to query\nuser_query = \"Can you explain what Compiler Construction is?\"\nresponse = chat_with_model(file_id, user_query)\n\nprint(\"Response:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:17:58.713506Z","iopub.execute_input":"2024-12-21T12:17:58.713808Z","iopub.status.idle":"2024-12-21T12:22:07.230578Z","shell.execute_reply.started":"2024-12-21T12:17:58.713785Z","shell.execute_reply":"2024-12-21T12:22:07.229384Z"}},"outputs":[{"name":"stdout","text":"Pinecone client initialized with environment: us-east-1\nContext for file 676594520c3eb8c3272efa2c: \n\nPage 1 of 1 \n \n \n \nDEPARTMENT OF COMPUTER SCIENCE \nUNIVERSITY OF ENGINEERING AND TECHNOLOGY, NEW C...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f08f5ce3264fa2b569c2107a322366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27aa8be46d5e4c659f7d771bc2c78cb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9a1ef1aad0421b86db27a8148a7497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d30a1f3dc6f4fc1a07e13fe078dcf14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2e6a75653d404789589c9d70e886f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee098216b41b41afa212d53e038a8c41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38f6015c0584b299cae36f72b13ac8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"188d76cd1376459a84692f7c17be6cbd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt: Question: Can you explain what Compiler Construction is?\nContext: \n\nPage 1 of 1 \n \n \n \nDEPARTMENT OF COMPUTER SCIENCE \nUNIVERSITY OF ENGINEERING AND TECHNOLOGY, NEW CAMPUS \nCS-471L Compiler Constructi...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-817e9d416673>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"676594520c3eb8c3272efa2c\"\u001b[0m  \u001b[0;31m# Replace with the file ID you want to query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Can you explain what Compiler Construction is?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Response:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-817e9d416673>\u001b[0m in \u001b[0;36mchat_with_model\u001b[0;34m(file_id, user_query)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Generate the response based on the user query and context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-817e9d416673>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(user_query, context, model)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Question: {user_query}\\nContext: {context}\\nAnswer:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt: {prompt[:200]}...\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Displaying first 200 chars of the prompt for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     def preprocess(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             )\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0muse_dynamic_cache_by_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_generated_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_default_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m         \u001b[0;31m# 7. determine generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids_length\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0minput_ids_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_input_ids\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1267\u001b[0m                 \u001b[0;34mf\"Input length of {input_ids_string} is {input_ids_length}, but `max_length` is set to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;34mf\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 150, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."],"ename":"ValueError","evalue":"Input length of input_ids is 150, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implemented Ngrok and CORS on top of the Flask API","metadata":{}},{"cell_type":"code","source":"import os\nimport threading\nimport time\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS  # Import CORS to handle cross-origin requests\nfrom pyngrok import ngrok  # Import ngrok\nfrom dotenv import load_dotenv  # Import dotenv to load .env file\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Enable CORS for all routes\nCORS(app)\n\n# Health check endpoint with request body and logging\n@app.route('/health', methods=['GET', 'POST'])\ndef health_check():\n    try:\n        # Log the request data and ID (if present)\n        if request.method == 'POST':\n            data = request.json\n            print(\"Received request data:\", data)\n            id_value = data.get(\"id\", \"No ID provided\")\n            print(\"Received ID:\", id_value)\n        else:\n            print(\"Received request data:\", request.args)\n        \n        # Respond with a Good message\n        return jsonify({\"status\": \"Flask server is running great!\"})\n    except Exception as e:\n        # Catch any exceptions and respond with an error message\n        print(f\"Error: {str(e)}\")\n        return jsonify({\"status\": \"Error occurred while processing the request.\"}), 500\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\n# Function to run Flask server\ndef run_flask():\n    print(\"Flask server is listening on port 5000...\")\n    app.run(host=\"0.0.0.0\", port=5000)  # Running on port 5000\n\n# Function to start ngrok and expose the Flask server\ndef run_ngrok():\n    # Authenticate ngrok using the API key from the .env file\n    ngrok_auth_token = os.getenv(\"NGROK_AUTHTOKEN\")\n    if ngrok_auth_token:\n        ngrok.set_auth_token(ngrok_auth_token)\n        print(\"Ngrok authenticated successfully.\")\n    else:\n        print(\"Ngrok authentication failed. Please check your API key.\")\n        return None\n\n    # Open a ngrok tunnel to the Flask app on port 5000\n    public_url = ngrok.connect(5000)\n    print(f\"Ngrok public URL: {public_url}\")\n    return public_url\n\n# Start Flask server in a separate thread to keep the notebook running\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True  # Allow thread to terminate when the program exits\nflask_thread.start()\n\n# Start ngrok and print the URL where Flask is accessible\npublic_url = run_ngrok()\n\n# Keep the notebook running to maintain the server\nwhile True:\n    time.sleep(1)  # Just keeps the notebook running\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T10:50:14.800325Z","iopub.execute_input":"2024-12-21T10:50:14.800689Z","iopub.status.idle":"2024-12-21T10:52:03.417907Z","shell.execute_reply.started":"2024-12-21T10:50:14.80065Z","shell.execute_reply":"2024-12-21T10:52:03.416792Z"}},"outputs":[{"name":"stdout","text":"Flask server is listening on port 5000...\n * Serving Flask app '__main__'\n * Debug mode: off\nNgrok authenticated successfully.\nNgrok public URL: NgrokTunnel: \"https://c89e-34-71-204-254.ngrok-free.app\" -> \"http://localhost:5000\"\nReceived request data: {'id': 12345}\nReceived ID: 12345\nReceived request data: {'id': 12345, 'url': 'httpsasdfadsf'}\nReceived ID: 12345\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f7dd4776d875>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Keep the notebook running to maintain the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just keeps the notebook running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# *Working Prototype for flask listening***","metadata":{}},{"cell_type":"code","source":"import os\nfrom flask import Flask, request, jsonify\nimport threading\nimport time\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Health check endpoint\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({\"status\": \"Flask server is running Greate!\"})\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\n# Function to run Flask server\ndef run_flask():\n    print(\"Flask server is listening on port 8000...\")\n    app.run(host=\"0.0.0.0\", port=9000)  # Running on port 8000\n\n# Start Flask server in a separate thread to keep the notebook running\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True  # Allow thread to terminate when the program exits\nflask_thread.start()\n\n# Print out the URL where Flask is running\nprint(\"Flask server should be accessible at: http://0.0.0.0:8000\")\n\n# Keep the notebook running to maintain the server\nwhile True:\n    time.sleep(1)  # Just keeps the notebook running\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How to send request from the JavaScript Server Side ","metadata":{}},{"cell_type":"code","source":"async function checkFlaskHealth() {\n    try {\n        // Replace with the ngrok URL provided by the Flask app\n        const apiUrl = 'https://c89e-34-71-204-254.ngrok-free.app/health';\n\n        // Prepare the request body with an 'id'\n        const requestBody = {\n            id: 12345, // Example ID, you can change this to anything\n            url: \"httpsasdfadsf\"\n        };\n\n        // Make a POST request to the /health endpoint with the request body\n        const response = await fetch(apiUrl, {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify(requestBody) // Send the request body as JSON\n        });\n\n        // Parse the JSON response\n        const data = await response.json();\n\n        // Log the response to the console\n        console.log(\"Response from Flask server:\", data);\n        \n        // Check the status of the response\n        if (data.status === \"Flask server is running great!\") {\n            console.log(\"Flask server is up and running!\");\n        } else {\n            console.log(\"Flask server is down or returned an unexpected response.\");\n        }\n    } catch (error) {\n        console.error(\"Error while making the request:\", error);\n    }\n}\n\n// Call the function to check the Flask health endpoint\ncheckFlaskHealth();\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T10:52:03.418547Z","iopub.status.idle":"2024-12-21T10:52:03.418821Z","shell.execute_reply":"2024-12-21T10:52:03.418711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}